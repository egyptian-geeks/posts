---
message: "Any recommendations for/experience with data persistence and exchange format
  (Thrift, Protobuff and Avro) and if anyone can shed the light of their integration
  with Parquet. \n\nThe use case is similar to a data pipeline and internal APIs communication."
from:
  name: Adel K. Khalil
  id: '10159958402365584'
type: status
created_time: '2018-01-16T17:30:36+0000'
updated_time: '2018-01-17T21:57:45+0000'
permalink_url: https://www.facebook.com/groups/egyptian.geeks/permalink/1724198497619851/
id: '172338516139198_1724198497619851'
reactions:
  data:
  - id: '1999765416908747'
    name: Ahmed Khalifa
    type: LIKE
  paging:
    cursors:
      before: TVRBd01EQTJNalU1TWpNNE9EVXlPakUxTVRZAeE1qTTVPVFk2TWpVME1EazJNVFl4TXc9PQZDZD
      after: TVRBd01EQTJNalU1TWpNNE9EVXlPakUxTVRZAeE1qTTVPVFk2TWpVME1EazJNVFl4TXc9PQZDZD
comments:
  data:
  - created_time: '2018-01-16T19:11:57+0000'
    from:
      name: Mohamed Elsherif
      id: '10160105091405314'
    message: "I think one main design goal for Avro was data persistence, since Thrift
      wasn't necessary Hadoop friendly, Avro is more flexible since Readers can use
      a different schema than Writers, so even if writer schema changed at some point,
      you could still read the older data using the older reader schema, \nSo in the
      context of data persistence on Hadoop for example, if you ar dumping a lot of
      data at one point, you can also include the writer's schema with it, so next
      time you are trying to read you can load up the schema as a reader schema and
      work off of it, and I think Avro supports this on Hadoop with a specific file
      format (object container files)\nAs for Parquet integration I'm sorry I don't
      know anything about it."
    id: '1724271240945910'
  - created_time: '2018-01-17T02:54:20+0000'
    from:
      name: Yahia Zakaria
      id: '10155011766866836'
    message: |-
      Personally, I used Protobuff before for message serialization between multiple nodes (around 60 nodes) and the performance was good. Hadoop 2.0 (a.k.a YARN) uses protobuf for serializing many messages/hearbeats per second between the node managers, application master, and the resource manager, so I believe protobuf has a good performance. I don't how it works with Parquet.  Also, I found this answer that compares  Thrift and Protobuff  They both offer many of the same features; however, there are some differences:

      Thrift supports 'exceptions'
      Protocol Buffers have much better documentation/examples
      Thrift has a builtin Set type
      Protocol Buffers allow "extensions" - you can extend an external proto to add extra fields, while still allowing external code to operate on the values. There is no way to do this in Thrift
      I find Protocol Buffers much easier to read
      Basically, they are fairly equivalent (with Protocol Buffers slightly more efficient from what I have read). https://stackoverflow.com/questions/69316/biggest-differences-of-thrift-vs-protocol-buffers
    id: '1724533240919710'
  - created_time: '2018-01-17T21:57:44+0000'
    from:
      name: Mohamed Radwan
      id: '10159976552365192'
    message: We use thrift to write to flume-ng to write to kafka
    id: '1725307060842328'
  paging:
    cursors:
      before: WTI5dGJXVnVkRjlqZAFhKemIzSTZANVGN5TkRJM01USTBNRGswTlRreE1Eb3hOVEUyTVRJNU9URTQZD
      after: WTI5dGJXVnVkRjlqZAFhKemIzSTZANVGN5TlRNd056QTJNRGcwTWpNeU9Eb3hOVEUyTWpJMk1qWTAZD
